{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvD1T2bqoiub"
   },
   "outputs": [],
   "source": [
    "%cd \"/content/drive/My Drive\"\n",
    "! pip install transformers\n",
    "# ! pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNSTU44BoEMp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "VOCAB = ('<PAD>', 'O', 'I-LOC', 'B-PERS', 'I-PERS', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG')\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
    "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}\n",
    "\n",
    "class NerDataset(data.Dataset):\n",
    "    def __init__(self, fpath):\n",
    "        \"\"\"\n",
    "        fpath: [train|valid|test].txt\n",
    "        \"\"\"\n",
    "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split(\"\\t\")[0] for line in entry.splitlines() if line]\n",
    "            tags = ([line.split(\"\\t\")[-1] for line in entry.splitlines() if line])\n",
    "            if len(words) == len(tags):\n",
    "                sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "                tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "\n",
    "        for i in range(len(y) - len(x)):\n",
    "            x.append(x[-1])\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96MBUPewoT-L"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, top_rnns=False, vocab_size=None, device='cpu', finetuning=False):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.top_rnns=top_rnns\n",
    "        if top_rnns:\n",
    "            self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768//2, batch_first=True)\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.finetuning = finetuning\n",
    "\n",
    "    def forward(self, x, y, ):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "\n",
    "        Returns\n",
    "        enc: (N, T, VOCAB)\n",
    "        '''\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        if self.training and self.finetuning:\n",
    "            # print(\"->bert.train()\")\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "\n",
    "        if self.top_rnns:\n",
    "            enc, _ = self.rnn(enc)\n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1ZHNcTHodeg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i==0:\n",
    "            print(\"=====sanity check======\")\n",
    "            print(\"words:\", words[0])\n",
    "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
    "            print(\"is_heads:\", is_heads[0])\n",
    "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
    "            print(\"tags:\", tags[0])\n",
    "            print(\"seqlen:\", seqlens[0])\n",
    "            print(\"=======================\")\n",
    "\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")\n",
    "\n",
    "def eval(model, iterator, f):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"temp\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open(\"temp\", 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    num_proposed = len(y_pred[y_pred>1])\n",
    "    num_correct = (np.logical_and(y_true==y_pred, y_true>1)).astype(np.int).sum()\n",
    "    num_gold = len(y_true[y_true>1])\n",
    "\n",
    "    print(f\"num_proposed:{num_proposed}\")\n",
    "    print(f\"num_correct:{num_correct}\")\n",
    "    print(f\"num_gold:{num_gold}\")\n",
    "    try:\n",
    "        precision = num_correct / num_proposed\n",
    "    except ZeroDivisionError:\n",
    "        precision = 1.0\n",
    "\n",
    "    try:\n",
    "        recall = num_correct / num_gold\n",
    "    except ZeroDivisionError:\n",
    "        recall = 1.0\n",
    "\n",
    "    try:\n",
    "        f1 = 2*precision*recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        if precision*recall==0:\n",
    "            f1=1.0\n",
    "        else:\n",
    "            f1=0\n",
    "\n",
    "    final = f + \".P%.2f_R%.2f_F%.2f\" %(precision, recall, f1)\n",
    "    with open(final, 'w') as fout:\n",
    "        result = open(\"temp\", \"r\").read()\n",
    "        fout.write(f\"{result}\\n\")\n",
    "\n",
    "        fout.write(f\"precision={precision}\\n\")\n",
    "        fout.write(f\"recall={recall}\\n\")\n",
    "        fout.write(f\"f1={f1}\\n\")\n",
    "\n",
    "    os.remove(\"temp\")\n",
    "\n",
    "    print(\"precision=%.2f\"%precision)\n",
    "    print(\"recall=%.2f\"%recall)\n",
    "    print(\"f1=%.2f\"%f1)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "cQKEs998l1cW",
    "outputId": "e5976fc7-ad8b-46a4-ac32-68740d30c333"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "lr = 0.0001\n",
    "n_epochs = 30\n",
    "finetuning = False\n",
    "top_rnns = True\n",
    "logdir   = \"checkpoints/01\"\n",
    "trainset = \"data/train.txt\"\n",
    "validset = \"data/valid.txt\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Net(top_rnns, len(VOCAB), device, finetuning).cuda()\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "train_dataset = NerDataset(trainset)\n",
    "eval_dataset = NerDataset(validset)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=pad)\n",
    "eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=4,\n",
    "                                collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train(model, train_iter, optimizer, criterion)\n",
    "\n",
    "    print(f\"=========eval at epoch={epoch}=========\")\n",
    "    if not os.path.exists(logdir): os.makedirs(logdir)\n",
    "    fname = os.path.join(logdir, str(epoch))\n",
    "    precision, recall, f1 = eval(model, eval_iter, fname)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{fname}.pt\")\n",
    "    print(f\"weights were saved to {fname}.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_ner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
